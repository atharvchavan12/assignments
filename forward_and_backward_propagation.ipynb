{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ae5d61-b11f-42b2-88d5-4137c9fa2b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nForward propagation is the process of passing an input through a neural network to produce an output. It involves feeding the input data through each layer of the network, applying activation functions to introduce non-linearity, and ultimately generating a prediction.\\nHere's a breakdown of the steps involved in forward propagation:\\n1.\\tInput Layer: The input data is fed into the input layer of the neural network.\\n2.\\tHidden Layers (if any): The input data is passed through one or more hidden layers. \\no\\tLinear Combination: The weighted sum of the inputs from the previous layer is calculated.\\no\\tActivation Function: An activation function (e.g., sigmoid, ReLU, tanh) is applied to introduce non-linearity.\\n3.\\tOutput Layer: The final layer of the network produces the output. \\no\\tLinear Combination: The weighted sum of the inputs from the previous layer is calculated.\\no\\tActivation Function: An activation function is applied, often depending on the task (e.g., softmax for classification).\\nIn essence, forward propagation is the process of moving data forward through the layers of a neural network to generate a prediction.\\nKey points to remember:\\n•\\tNon-linearity: Activation functions are essential for introducing non-linearity, allowing the network to learn complex patterns.\\n•\\tWeighted Sum: The weights assigned to each connection determine the importance of the input features.\\n•\\tOutput: The final output of the network is a prediction based on the input data and the learned weights.\\nBy understanding forward propagation, you can grasp the fundamental mechanism of how neural networks process information and make predictions.\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.' Explain the concept of forward propagation in a neural network'\n",
    "\"\"\"\n",
    "Forward propagation is the process of passing an input through a neural network to produce an output. It involves feeding the input data through each layer of the network, applying activation functions to introduce non-linearity, and ultimately generating a prediction.\n",
    "Here's a breakdown of the steps involved in forward propagation:\n",
    "1.\tInput Layer: The input data is fed into the input layer of the neural network.\n",
    "2.\tHidden Layers (if any): The input data is passed through one or more hidden layers. \n",
    "o\tLinear Combination: The weighted sum of the inputs from the previous layer is calculated.\n",
    "o\tActivation Function: An activation function (e.g., sigmoid, ReLU, tanh) is applied to introduce non-linearity.\n",
    "3.\tOutput Layer: The final layer of the network produces the output. \n",
    "o\tLinear Combination: The weighted sum of the inputs from the previous layer is calculated.\n",
    "o\tActivation Function: An activation function is applied, often depending on the task (e.g., softmax for classification).\n",
    "In essence, forward propagation is the process of moving data forward through the layers of a neural network to generate a prediction.\n",
    "Key points to remember:\n",
    "•\tNon-linearity: Activation functions are essential for introducing non-linearity, allowing the network to learn complex patterns.\n",
    "•\tWeighted Sum: The weights assigned to each connection determine the importance of the input features.\n",
    "•\tOutput: The final output of the network is a prediction based on the input data and the learned weights.\n",
    "By understanding forward propagation, you can grasp the fundamental mechanism of how neural networks process information and make predictions.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d1b3d9b-54a3-4a22-a35f-b7fd2e449d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe activation function in forward propagation serves several key purposes in neural networks:\\n1.\\tNon-linearity: The activation function introduces non-linearity into the model, which allows the neural network to learn and model complex patterns. Without it, the network would be a linear combination of inputs, limiting its ability to solve non-linear problems.\\n2.\\tThresholding: In some cases (e.g., step function or ReLU), the activation function helps decide whether a neuron should \"activate\" or not, based on the input.\\n3.\\tNormalization: Some activation functions (like sigmoid and tanh) map the input to a specific range (e.g., [0, 1] or [-1, 1]), which helps in stabilizing the learning process.\\n4.\\tDifferentiability: For backpropagation to work (updating weights), the activation function must be differentiable. Functions like ReLU, sigmoid, and tanh allow the gradient to be computed, enabling the training of the network.\\nOverall, the activation function helps the network capture non-linear relationships and transforms the raw input data into meaningful representations for further processing.\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. What is the purpose of the activation function in forward propagation\n",
    "\"\"\"\n",
    "The activation function in forward propagation serves several key purposes in neural networks:\n",
    "1.\tNon-linearity: The activation function introduces non-linearity into the model, which allows the neural network to learn and model complex patterns. Without it, the network would be a linear combination of inputs, limiting its ability to solve non-linear problems.\n",
    "2.\tThresholding: In some cases (e.g., step function or ReLU), the activation function helps decide whether a neuron should \"activate\" or not, based on the input.\n",
    "3.\tNormalization: Some activation functions (like sigmoid and tanh) map the input to a specific range (e.g., [0, 1] or [-1, 1]), which helps in stabilizing the learning process.\n",
    "4.\tDifferentiability: For backpropagation to work (updating weights), the activation function must be differentiable. Functions like ReLU, sigmoid, and tanh allow the gradient to be computed, enabling the training of the network.\n",
    "Overall, the activation function helps the network capture non-linear relationships and transforms the raw input data into meaningful representations for further processing.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93420683-88f1-4f1a-91a3-4edf7fd14869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBackward Propagation (Backpropagation)\\nBackward propagation is the process of calculating the gradients of the loss function with respect to the network's weights and biases. These gradients are used to update the weights and biases during training, allowing the network to learn and improve its performance.   \\nSteps Involved in Backpropagation:\\n1.\\tForward Pass:\\no\\tCalculate the output of the network for the given input using forward propagation.\\n2.\\tCompute Loss:\\no\\tCalculate the difference between the predicted output and the actual target using a suitable loss function (e.g., mean squared error, cross-entropy).\\n3.\\tCalculate Gradients:\\no\\tUse the chain rule to calculate the gradients of the loss function with respect to the weights and biases of each layer.\\no\\tStart from the output layer and work backward, recursively applying the chain rule to compute the gradients for each layer.\\n4.\\tUpdate Weights and Biases:\\no\\tAdjust the weights and biases based on the calculated gradients using an optimization algorithm like gradient descent, stochastic gradient descent, or Adam.\\nKey Points:\\n•\\tChain Rule: The chain rule is a fundamental calculus rule that allows us to calculate the derivative of a composite function. In backpropagation, it is used to recursively compute the gradients of the loss function with respect to the weights and biases of each layer.\\n•\\tGradient Descent: Gradient descent is a common optimization algorithm used to update the weights and biases based on the calculated gradients. It involves moving in the direction of the steepest descent of the loss function.\\n•\\tIterative Process: Backpropagation is an iterative process. The network is trained over multiple epochs, with the weights and biases being updated after each epoch.\\nBy understanding the steps involved in backpropagation, you can grasp how neural networks learn to improve their predictions by adjusting their parameters based on the feedback from the loss function.\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.\tDescribe the steps involved in the backward propagation (backpropagation) algorithm'.\n",
    "\"\"\"\n",
    "Backward Propagation (Backpropagation)\n",
    "Backward propagation is the process of calculating the gradients of the loss function with respect to the network's weights and biases. These gradients are used to update the weights and biases during training, allowing the network to learn and improve its performance.   \n",
    "Steps Involved in Backpropagation:\n",
    "1.\tForward Pass:\n",
    "o\tCalculate the output of the network for the given input using forward propagation.\n",
    "2.\tCompute Loss:\n",
    "o\tCalculate the difference between the predicted output and the actual target using a suitable loss function (e.g., mean squared error, cross-entropy).\n",
    "3.\tCalculate Gradients:\n",
    "o\tUse the chain rule to calculate the gradients of the loss function with respect to the weights and biases of each layer.\n",
    "o\tStart from the output layer and work backward, recursively applying the chain rule to compute the gradients for each layer.\n",
    "4.\tUpdate Weights and Biases:\n",
    "o\tAdjust the weights and biases based on the calculated gradients using an optimization algorithm like gradient descent, stochastic gradient descent, or Adam.\n",
    "Key Points:\n",
    "•\tChain Rule: The chain rule is a fundamental calculus rule that allows us to calculate the derivative of a composite function. In backpropagation, it is used to recursively compute the gradients of the loss function with respect to the weights and biases of each layer.\n",
    "•\tGradient Descent: Gradient descent is a common optimization algorithm used to update the weights and biases based on the calculated gradients. It involves moving in the direction of the steepest descent of the loss function.\n",
    "•\tIterative Process: Backpropagation is an iterative process. The network is trained over multiple epochs, with the weights and biases being updated after each epoch.\n",
    "By understanding the steps involved in backpropagation, you can grasp how neural networks learn to improve their predictions by adjusting their parameters based on the feedback from the loss function.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b933410-c759-4967-8521-f5b5060a6ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe chain rule in backpropagation is used to recursively calculate the gradients of the loss function with respect to the weights and biases of each layer in a neural network.\\nHere\\'s a breakdown of why the chain rule is essential:\\n1.\\tComposite Functions: Neural networks are composed of multiple layers, each of which can be considered a function of the previous layer\\'s output. This creates a chain of composite functions.\\n2.\\tGradient Calculation: To update the weights and biases, we need to calculate the gradients of the loss function with respect to these parameters.\\n3.\\tChain Rule Application: The chain rule allows us to break down the calculation of the gradient into smaller, more manageable steps. By applying the chain rule, we can recursively compute the gradients for each layer, starting from the output layer and working backward.\\n4.\\tEfficient Calculation: The chain rule provides an efficient way to compute the gradients, avoiding the need to calculate them directly for each layer.\\nIn essence, the chain rule enables us to efficiently calculate the gradients of a complex neural network, allowing the network to learn and improve.\\n\\n\\n5.\\' Implement the forward propagation process for a simple neural network with one hidden layer using NumPy.\\n\\nHere\\'s a simple implementation of the forward propagation process for a neural network with one hidden layer using NumPy. This example will assume a basic architecture where the input layer has a specified number of neurons, a hidden layer has a specified number of neurons, and an output layer has a single neuron.\\nSimple Neural Network Architecture:\\n•\\tInput Layer: nnn neurons\\n•\\tHidden Layer: mmm neurons\\n•\\tOutput Layer: 1 neuron\\nActivation Function:\\nWe\\'ll use the sigmoid activation function for both the hidden and output layers.\\nimport numpy as np\\n\\n# Sigmoid activation function\\ndef sigmoid(x):\\n    return 1 / (1 + np.exp(-x))\\n\\n# Derivative of the sigmoid function\\ndef sigmoid_derivative(x):\\n    return x * (1 - x)\\n\\n# Forward propagation function\\ndef forward_propagation(X, weights_input_hidden, weights_hidden_output):\\n    # Layer 1 (Hidden Layer)\\n    z_hidden = np.dot(X, weights_input_hidden)  # Weighted sum\\n    a_hidden = sigmoid(z_hidden)                 # Activation\\n\\n    # Layer 2 (Output Layer)\\n    z_output = np.dot(a_hidden, weights_hidden_output)  # Weighted sum\\n    a_output = sigmoid(z_output)                        # Activation\\n\\n    return a_hidden, a_output\\n\\n# Example usage\\nif __name__ == \"__main__\":\\n    # Input data: 4 samples, 3 features\\n    X = np.array([[0, 0, 1],\\n                  [1, 0, 1],\\n                  [0, 1, 1],\\n                  [1, 1, 1]])\\n\\n    # Random weights initialization\\n    np.random.seed(42)  # For reproducibility\\n    weights_input_hidden = np.random.rand(3, 4)  # 3 inputs to 4 hidden neurons\\n    weights_hidden_output = np.random.rand(4, 1)  # 4 hidden neurons to 1 output\\n\\n    # Forward propagation\\n    a_hidden, a_output = forward_propagation(X, weights_input_hidden, weights_hidden_output)\\n\\n    print(\"Hidden Layer Activations:\\n\", a_hidden)\\n    print(\"Output Layer Activations:\\n\", a_output)\\n\\nExplanation:\\n1.\\tSigmoid Function: The sigmoid activation function is defined to squash the output to the range (0, 1).\\n2.\\tWeights Initialization: Weights for the input to hidden layer and hidden to output layer are randomly initialized. The shapes of the weights correspond to the number of neurons in the respective layers.\\n3.\\tForward Propagation Function:\\no\\tThe function computes the weighted sum (z_hidden) for the hidden layer and applies the sigmoid activation to get the activations (a_hidden).\\no\\tThen, it computes the weighted sum (z_output) for the output layer and applies the sigmoid activation to get the final output (a_output).\\n4.\\tExample Usage: We define some sample input data and run the forward propagation to see the hidden and output layer activations.\\nYou can adjust the number of neurons and input features as needed for your specific use case.\\n\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.' What is the purpose of the chain rule in backpropagation\n",
    "\"\"\"\n",
    "The chain rule in backpropagation is used to recursively calculate the gradients of the loss function with respect to the weights and biases of each layer in a neural network.\n",
    "Here's a breakdown of why the chain rule is essential:\n",
    "1.\tComposite Functions: Neural networks are composed of multiple layers, each of which can be considered a function of the previous layer's output. This creates a chain of composite functions.\n",
    "2.\tGradient Calculation: To update the weights and biases, we need to calculate the gradients of the loss function with respect to these parameters.\n",
    "3.\tChain Rule Application: The chain rule allows us to break down the calculation of the gradient into smaller, more manageable steps. By applying the chain rule, we can recursively compute the gradients for each layer, starting from the output layer and working backward.\n",
    "4.\tEfficient Calculation: The chain rule provides an efficient way to compute the gradients, avoiding the need to calculate them directly for each layer.\n",
    "In essence, the chain rule enables us to efficiently calculate the gradients of a complex neural network, allowing the network to learn and improve.\n",
    "\n",
    "\n",
    "5.' Implement the forward propagation process for a simple neural network with one hidden layer using NumPy.\n",
    "\n",
    "Here's a simple implementation of the forward propagation process for a neural network with one hidden layer using NumPy. This example will assume a basic architecture where the input layer has a specified number of neurons, a hidden layer has a specified number of neurons, and an output layer has a single neuron.\n",
    "Simple Neural Network Architecture:\n",
    "•\tInput Layer: nnn neurons\n",
    "•\tHidden Layer: mmm neurons\n",
    "•\tOutput Layer: 1 neuron\n",
    "Activation Function:\n",
    "We'll use the sigmoid activation function for both the hidden and output layers.\n",
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Forward propagation function\n",
    "def forward_propagation(X, weights_input_hidden, weights_hidden_output):\n",
    "    # Layer 1 (Hidden Layer)\n",
    "    z_hidden = np.dot(X, weights_input_hidden)  # Weighted sum\n",
    "    a_hidden = sigmoid(z_hidden)                 # Activation\n",
    "\n",
    "    # Layer 2 (Output Layer)\n",
    "    z_output = np.dot(a_hidden, weights_hidden_output)  # Weighted sum\n",
    "    a_output = sigmoid(z_output)                        # Activation\n",
    "\n",
    "    return a_hidden, a_output\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input data: 4 samples, 3 features\n",
    "    X = np.array([[0, 0, 1],\n",
    "                  [1, 0, 1],\n",
    "                  [0, 1, 1],\n",
    "                  [1, 1, 1]])\n",
    "\n",
    "    # Random weights initialization\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    weights_input_hidden = np.random.rand(3, 4)  # 3 inputs to 4 hidden neurons\n",
    "    weights_hidden_output = np.random.rand(4, 1)  # 4 hidden neurons to 1 output\n",
    "\n",
    "    # Forward propagation\n",
    "    a_hidden, a_output = forward_propagation(X, weights_input_hidden, weights_hidden_output)\n",
    "\n",
    "    print(\"Hidden Layer Activations:\\n\", a_hidden)\n",
    "    print(\"Output Layer Activations:\\n\", a_output)\n",
    "\n",
    "Explanation:\n",
    "1.\tSigmoid Function: The sigmoid activation function is defined to squash the output to the range (0, 1).\n",
    "2.\tWeights Initialization: Weights for the input to hidden layer and hidden to output layer are randomly initialized. The shapes of the weights correspond to the number of neurons in the respective layers.\n",
    "3.\tForward Propagation Function:\n",
    "o\tThe function computes the weighted sum (z_hidden) for the hidden layer and applies the sigmoid activation to get the activations (a_hidden).\n",
    "o\tThen, it computes the weighted sum (z_output) for the output layer and applies the sigmoid activation to get the final output (a_output).\n",
    "4.\tExample Usage: We define some sample input data and run the forward propagation to see the hidden and output layer activations.\n",
    "You can adjust the number of neurons and input features as needed for your specific use case.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f1eff-f23f-423e-a77e-5e2fdf6b4839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
